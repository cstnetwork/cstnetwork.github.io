<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Constraint-Aware Feature Learning for Parametric Point Cloud">
  <meta name="keywords" content="point cloud, 3D computer vision, deep learning, CAD shapes">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Constraint-Aware Feature Learning for Parametric Point Cloud</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/index.js"></script>
<style>
#interactive {
  position: relative;
  display: inline-block;
  width: 768px;
  aspect-ratio: 16/9;
  max-width: 100%;
}
#interactive canvas, #interactive svg, #interactive #glfailed {
  display: block;
  position: absolute;
  width: 100%;
  height: 100%;
  touch-action: none;
}
#interactive svg {
  pointer-events: none;
}
#interactive #glfailed {
  color: #f88;
  background: black;
  display: none;
}
.load img {
  width: 128px;
  height: 72px;
  margin-right: 4px;
}
</style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <h1 class="title is-1 publication-title">Constraint Learning</h1>
          <h1 class="title is-1 publication-title">for Parametric Point Cloud</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Xi Cheng</a>,
            </span>
            <span class="author-block">
              <a href="">Ruiqi Lei</a>,
            </span>
            <span class="author-block">
              <a href="">Di Huang</a>,
            </span>
            <span class="author-block">
              <a href="">Zhichao Liao</a>,
            </span>
            <span class="author-block">
              <a href="">Fengyuan Piao</a>,
            </span>
            <span class="author-block">
              <a href="">Yan Chen</a>,
            </span>
            <span class="author-block">
              <a href="">Pingfa Feng</a>,
            </span>
            <span class="author-block">
              <a href="">Long Zeng</a><sup>&#9993</sup>
            </span>
            
          </div>

          <div class="is-size-5 publication-authors"> 
            <span class="author-block" style="font-size: smaller;"> &#9993 Corresponding authors </span> 
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China </span>
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2408.05966"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/cstnetwork/cstnet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://pan.baidu.com/s/15F_-jz_XnFuW65k5aqjPNg?pwd=1234"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-archive"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>


            </div>
          </div>

          <!-- Author Contributions Section -->

          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Parametric point clouds are sampled from CAD shapes and are becoming increasingly common in industrial manufacturing. Most existing CAD-specific deep learning methods only focus on geometric features, while overlooking constraints which are inherent and important in CAD shapes. This limits their ability to discern CAD shapes with similar appearance but different constraints. To tackle this challenge, we first analyze the constraint importance via a simple validation experiment. Then, we introduce a deep learning-friendly constraints representation with three vectorized components, and design a constraint-aware feature learning network (CstNet), which includes two stages. Stage 1 extracts constraint feature from B-Rep data or point cloud based on shape local information. It enables better generalization ability to unseen dataset after model pre-training. Stage 2 employs attention layers to adaptively adjust the weights of three constraints' components. It facilitates the effective utilization of constraints. In addition, we built the first multi-modal parametric-purpose dataset, i.e. Param20K, comprising about 20K shape instances of 75 classes. On this dataset, we performed the classification and rotation robustness experiments, and CstNet achieved 3.52% and 26.17% absolute improvements in instance accuracy over the state-of-the-art methods, respectively. To the best of our knowledge, CstNet is the first constraint-aware deep learning method tailored for parametric point cloud analysis in CAD domain.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- motivation Section -->
<section class="section" id="teaser image">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Motivation</h2>
      <div class="content has-text-justified">
        <p>
          Some similar CAD shapes serve different functions. The eccentric wheel has a rotation axis that is offset from its outer cylindrical surface, facilitating precise positioning or tension adjustment. In contrast, the flywheel has a coaxial rotation axis with its outer cylindrical surface and is used to store rotational kinetic energy. Although these shapes are challenging to distinguish visually, they can be easily differentiated from the perspective of constraints.
        </p>
      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>
                <div style="text-align: center;">
                  <img id="teaser" width="60%" src="static/images/motivation.png">     
                </div><br>
              </centering>
            </b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- validation Section -->
<section class="section" id="validation_image">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Idea Validation Experiments</h2>
      <div class="content has-text-justified">
        <p>
          Constraints are crucial for CAD shapes, can they be utilized to enhance deep learning methods' performance? 
          To answer above question, we designed the following validation experiments. The objective is to compare the prisms shown in left figure with cuboids. 
          In each experiment, only prisms with a specific angle are selected, creating a binary classification task between prisms and cuboids. 
          Consequently, eight independent experiments are conducted to cover all angles. As the prism’s angle approaches 90°, its resemblance to cuboids increases, making it progressively more challenging to distinguish them. 
          The constraint-aware model is built on PointNet++ backbone, this model predicts constraints and then adopts it for classification.
        </p>
        <p>
          The experimental results are presented in the right figure, which demonstrating the constraint enhanced the model's comprehension of CAD shapes.
        </p>

      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>
                <div style="text-align: center;">
                  <img id="prisms" width="100%" src="static/images/valid_experi.png">
                </div><br>
              </centering>
            </b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- framework Section -->
<section class="section" id="framework image">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Framework</h2>
      <div class="content has-text-justified">
        <p>
          The top side illustrates the overall architecture of CstNet, comprising two stages. 
          Stage 1 is designed for constraint acquisition. When B-Rep data is available, the CST-BRep module is utilized to extract constraints; otherwise, use the CST-PCD module. 
          Stage 2 performs constraint feature learning, facilitating a deeper understanding of CAD shapes. The bottom side presents the details of module design.
        </p>
      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>
                <div style="text-align: center;">
                  <img id="framework" width="100%" src="static/images/framework.png">     
                </div><br>
              </centering>
            </b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- dataset Section -->
<section class="section" id="dataset image">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Parametric20000 Dataset</h2>
      <div class="content has-text-justified">
        <p>
          Parametric20000 Dataset is a multi-modal CAD shapes dataset built by our team, which contains B-Rep, Mesh, and point cloud. 
          Most existing CAD shape datasets consist of mesh files. 
          While mesh files could approximate the appearance of CAD shapes, lack crucial boundary information. 
          In contrast, B-Rep data serves as the native representation of CAD shapes and is therefore more suitable for dataset construction. 
        </p>
      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>

                <div style="text-align: center;">
                  <img id="dataset models" width="65%" src="static/images/model_show.png">
                  <img id="dataset percentage" width="30%" src="static/images/data_percentage.png">
                  <img id="dataset statis" width="100%" src="static/images/data_distribute.png">
                </div>

              </centering>
            </b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- experiment Section -->
<section class="section" id="experiment image">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Experiments</h2>
      <div class="content has-text-justified">
        <p>
          We have conducted classification on MCB, Parametric20000, and segmentation on 360 Gallery.
        </p>
      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>
                <div style="text-align: center;">
                  <img id="experiment" width="100%" src="static/images/MCB.png">     
                </div><br>
              </centering>

              <centering>
                <div style="text-align: center;">
                  <img id="360" width="45%" src="static/images/360 gallery.png" style="float: left;">
                  <img id="ablation" width="45%" src="static/images/Ablation.png" style="float: right;">
                </div><br>
              </centering>

            </b>
          </p>
        </div>
      </div>
    </div>
  </div>


<!-- experiment Section -->
<section class="section" id="others image">

  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Others</h2>
    </div>
  </div>

  <p>
    Constraint prediction experiments.<br>
    ABC:
  </p>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>
                <div style="text-align: center;">
                  <img id="abc" width="70%" src="static/images/pred_of_abc3.png">     
                </div><br>
              </centering>
            </b>
          </p>
        </div>
      </div>
    </div>
  </div>

  <p>
    MCB:
  </p>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>
                <div style="text-align: center;">  
                  <img id="mcb_pred" width="70%" src="static/images/pred_of_mcb5.png">
                </div><br>
              </centering>
            </b>
          </p>
        </div>
      </div>
    </div>
  </div>

  <p>
    KNN and SurfaceKNN:
  </p>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            <b>
              <centering>
                <div style="text-align: center;">  
                  <img id="surfknn" width="80%" src="static/images/knnsurfknn.png">
                </div><br>
              </centering>
            </b>
          </p>
        </div>
      </div>
    </div>
  </div>


</section>

<br>
<section>
<br>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{liao2024freehand,
        title={Freehand Sketch Generation from Mechanical Components},
        author={Liao, Zhichao and Huang, Di and Fang, Heming and Ma, Yue and Piao, Fengyuan and Li, Xinghui and Zeng, Long and Feng, Pingfa},
        year={2024},
        eprint={2408.05966},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2408.05966},
      }
    </code></pre>
  </div>
</section>

</body>
</html>

